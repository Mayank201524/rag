# Retrieval-Augmented Generation (RAG) Pipeline with Pinecone & OpenAI

This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using OpenAI embeddings and Pinecone for document search and Q\&A. The pipeline allows you to ask questions about your own PDF documents and get context-aware answers generated by a GPT model.

---

## 🚀 Features

* **Document Ingestion & Chunking:**
  Loads and splits PDF files (or other text sources) into manageable text chunks for vectorization.
* **Embedding Generation:**
  Uses OpenAI’s embedding API to turn text chunks into vector representations.
* **Pinecone Vector Database:**
  Stores and indexes vectors for scalable, low-latency similarity search.
* **RAG Query Pipeline:**
  Retrieves relevant chunks from Pinecone and feeds them to an OpenAI GPT model for grounded, accurate answers.

---

## 🛠️ Dependencies

* `openai`
* `pinecone-client`
* `langchain_pinecone`
* `numpy`

Install everything via:

```bash
pip install openai pinecone-client langchain_pinecone numpy
```
## 📓 Usage

1. **Run cells sequentially:**

   * Chunk your documents
   * Generate embeddings
   * Store embeddings in Pinecone
   * Query your system using natural language

2. **Ask questions:**

   * Enter a query; the system retrieves top-matching text chunks and uses GPT to generate a contextual answer.

---

## ⚙️ Customization

* **Chunk Size:**
  Adjust chunk size and overlap for best retrieval results.
* **Prompt Design:**
  Modify prompts to GPT for improved answer style or completeness.
* **Retrieval Parameters:**
  Change number of retrieved chunks or try other OpenAI embedding models.
